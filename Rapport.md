minimum 20 pages, prÃ©sentant lâ€™Ã©tude des performances de tous les algorithmes et modÃ¨les sur la problÃ© matique choisie.

Prob : faire un model qui differencie et classes des images de divers vehicules(Voiture, camion, moto, vÃ©lo).
Repo Git : https://github.com/Nine-rem/PA-3IABD/tree/rendu2-Robin
Dataset : pour construire le dataset, il nous faut une grande quantitÃ© de photos prise avec des appareils diffÃ©rent.
    * Les photos doivent Ãªtre bien centrÃ©e et si possible pas d'autres types de vÃ©hicules prÃ©sent sur la photo.
    * Le vÃ©hicule doit Ãªtre bien reconnaissable (pas de photo de vÃ©hicule Ã  100m par exemple).
    * Bien mettre les images dans la bonne catÃ©gorie (pour trier les photos plus rapidement).

Notes:
    Dataset:
        il nous faut trouver un moyen de standardiser les images.

    LinearModel: 
     les situations oÃ¹ ton modÃ¨le linÃ©aire se trompe visiblement :
    - Soit les erreurs sont trop grandes (en rÃ©gression),
    - Soit le modÃ¨le classe mal (en classification),
    - Soit les rÃ©sultats ne collent pas Ã  la forme des donnÃ©es (ex :
      donnÃ©es non-linÃ©aires).


Performances:
    Algorithmes :
    ModÃ¨les:




11 mai 2025
## Objectif de lâ€™Ã©tape

Lâ€™objectif de cette Ã©tape est dâ€™implÃ©menter et tester les premiers modÃ¨les de Machine Learning sans utiliser de bibliothÃ¨ques externe. Les modÃ¨les devaient Ãªtre testÃ©s sur des jeux de donnÃ©es simples ainsi quâ€™une portion du dataset final.

- ImplÃ©mentation dâ€™un modÃ¨le linÃ©aire et dâ€™un PMC
- Mise en place de transformations non linÃ©aires
- Tests pour valider les apprentissages

## ModÃ¨les implÃ©mentÃ©s

### 1. ModÃ¨le LinÃ©aire
- ImplÃ©mentÃ© avec fonction dâ€™activation `sigmoid` pour la classification binaire.
- Optimisation par descente de gradient.
- TestÃ© dâ€™abord sur donnÃ©es jouets puis sur fichier `dataset.csv`.

### 2. Transformation Non LinÃ©aire
- Pour amÃ©liorer les performances du modÃ¨le linÃ©aire sur des donnÃ©es non linÃ©airement sÃ©parables.
- Transformation appliquÃ©e : `x â†’ [x, xÂ², sin(x)]`.

### 3. Perceptron Multi-Couches (PMC)
- ImplÃ©mentation dâ€™un MLP avec propagation avant et rÃ©tropropagation.
- Architecture flexible : `[input_size, hidden, output]`
- Fonction dâ€™activation : `sigmoid`
- TestÃ© sur donnÃ©es XOR (`pmc_dataset.csv`).

 
## ğŸ” Tests et rÃ©sultats

### ğŸ”¸ ModÃ¨le LinÃ©aire
Test : sÃ©paration de deux classes simulÃ©es  
RÃ©sultat : prÃ©dictions correctes avec une transformation `x, xÂ², sin(x)`

### ğŸ”¸ PMC (Perceptron Multi-Couche)
Test : apprentissage du XOR  
RÃ©sultat : convergence stable (prÃ©dictions correctes dans tolÃ©rance < 0.4)

### ğŸ”¸ Transformation non linÃ©aire
Test : vÃ©rification que les colonnes sont bien transformÃ©es (x, xÂ², sin(x))  
RÃ©sultat : exactitude numÃ©rique vÃ©rifiÃ©e




### Analyse
Le modÃ¨le linÃ©aire montre des limites claires sur les donnÃ©es XOR ou non sÃ©parables sans transformation.

Lâ€™ajout dâ€™une transformation non linÃ©aire amÃ©liore significativement les rÃ©sultats.

Le PMC surpasse facilement le modÃ¨le linÃ©aire dÃ¨s quâ€™il sâ€™agit de capter des relations complexes (non linÃ©aires).

Lâ€™architecture modulaire (lib sÃ©parÃ©e + tests en examples/).